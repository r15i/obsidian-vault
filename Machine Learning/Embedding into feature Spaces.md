

Define a **non-linear** mapping ğœ“ âˆ¶ ğ’³ â†’ â„± such that $F$ is a new Feature Space

$F$  is usually $R^{\,n}$ for some $n$ but can be an arbitrary Hilbert Space (even with $|F|  = \infty$)

$ğ‘† = ((ğ’™_ğŸ, ğ‘¦_1) ,â€¦ , (ğ’™_ğ’, ğ‘¦_ğ‘š))$  --Fâ†’ $ğ‘†^{'} = ( (ğœ“(ğ’™_ğŸ) ,ğ‘¦_1),â€¦ , (ğœ“ (ğ’™_ğ’) , ğ‘¦_ğ‘š))$





idea: 
- apply a non linear transformation to each point in the training set
- learn a linear predictor in the transformad space 
- make prediction for a new instance

SOLUTION:
Embedding into feature Spaces 
Define a non-linear mapping ğœ“ from the input space to a new (typically larger) space
1. Given a domain set ğ’³ and a learning task, find a mapping to a new feature space â„± ğœ“ âˆ¶ ğ’³ â†’ â„± â€¢ â„± is usually â„ ğ‘› for some n but can be an arbitrary Hilbert space (even of infinite size) 
2. Given a sequence of labeled examples ğ‘† = ( ğ’™ğŸ, ğ‘¦1 ,â€¦ , (ğ’™ğ’, ğ‘¦ğ‘š)) map them to ğ‘† = (ğœ“ ğ’™ğŸ , ğ‘¦1 ,â€¦ , (ğœ“ ğ’™ğ’ , ğ‘¦ğ‘š)) 
3. Train a linear predictor h over ğ‘†áˆ˜ 4. Predict the label of ğ’™ as â„(ğœ“ ğ’™ )

FIND ğœ“ (x) is a very computational intesive task that can be worked around with  [[Kernel-based learning#Kernel trick | kernel trick]]

