
Faster variant of [[Gradient Descent (GD)]]  which takes a random vector with expected value equal to the gradient direction instead of computing explicitly the 
gradient

![[Pasted image 20231201114052.png]]

![[Pasted image 20231201114310.png]]
faster but nosier 

# Application of SGD

# 1. [[Empirical Risk Minimization (ERM)|Risk minimization]]
![[Pasted image 20231201115433.png]]
	Minimize [[True Error]] $L_{D}$ directly 
	find an ubiased estimate of the gradient of $L_{D}$
	

# 2. [[Regularized Loss minimization (RLM)]]
# 3. [[Support Vector Machines (SVM)]]
# 4. [[Neural Networks (NN)]]

